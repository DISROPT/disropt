

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>(Sub)gradient based Algorithms &mdash; disropt 0.0.6 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dual and Primal/Dual algorithms" href="disropt.algorithms.dual.html" />
    <link rel="prev" title="Consensus Algorithms" href="disropt.algorithms.consensus.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> disropt
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Home</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/index.html">Quick start</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/index.html">Tutorial</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="disropt.agents.html"> Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="disropt.communicators.html"> Communicator</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="disropt.algorithms.html"> Algorithms</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="disropt.algorithms.consensus.html">Consensus</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">(Sub)gradient algorithms</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#distributed-projected-sub-gradient-method">Distributed projected (Sub)gradient Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#randomized-block-sub-gradient-method">Randomized Block (Sub)gradient Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-gradient-tracking">Distributed Gradient Tracking</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-gradient-tracking-over-directed-unbalanced-graphs">Distributed Gradient Tracking (over directed, unbalanced graphs)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="disropt.algorithms.dual.html">Dual and Primal/Dual</a></li>
<li class="toctree-l3"><a class="reference internal" href="disropt.algorithms.setmembership.html">Set membership algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="disropt.algorithms.misc.html">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="disropt.functions.html"> Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="disropt.constraints.html"> Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="disropt.problems.html"> Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="disropt.utils.html"> Utilities</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/index.html">Advanced features</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../acknowledgements/index.html">Acknowledgements</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">disropt</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">API Documentation</a> &raquo;</li>
        
          <li><a href="disropt.algorithms.html">Algorithms</a> &raquo;</li>
        
      <li>(Sub)gradient based Algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api_documentation/disropt.algorithms.subgradient.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sub-gradient-based-algorithms">
<h1>(Sub)gradient based Algorithms<a class="headerlink" href="#sub-gradient-based-algorithms" title="Permalink to this headline">¶</a></h1>
<div class="section" id="distributed-projected-sub-gradient-method">
<h2>Distributed projected (Sub)gradient Method<a class="headerlink" href="#distributed-projected-sub-gradient-method" title="Permalink to this headline">¶</a></h2>
<span class="target" id="alg-subgradient"></span><dl class="class">
<dt id="disropt.algorithms.subgradient.SubgradientMethod">
<em class="property">class </em><code class="sig-prename descclassname">disropt.algorithms.subgradient.</code><code class="sig-name descname">SubgradientMethod</code><span class="sig-paren">(</span><em class="sig-param">agent</em>, <em class="sig-param">initial_condition</em>, <em class="sig-param">enable_log=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/subgradient.html#SubgradientMethod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.subgradient.SubgradientMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="disropt.algorithms.consensus.html#disropt.algorithms.consensus.Consensus" title="disropt.algorithms.consensus.Consensus"><code class="xref py py-class docutils literal notranslate"><span class="pre">disropt.algorithms.consensus.Consensus</span></code></a></p>
<p>Distributed projected (sub)gradient method.</p>
<p>From the perspective of agent <span class="math notranslate nohighlight">\(i\)</span> the algorithm works as follows. For <span class="math notranslate nohighlight">\(k=0,1,\dots\)</span></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned} y_i^{k} &amp;= \sum_{j=1}^N w_{ij} x_j^k\\x_i^{k+1} &amp;= \Pi_{X}[y_i^k - \alpha^k \tilde{\nabla} f_i(y_i^k)]\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(x_i,y_i\in\mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(X\subseteq\mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(\alpha^k\)</span> is a positive stepsize, <span class="math notranslate nohighlight">\(w_{ij}\)</span> denotes the weight assigned by agent <span class="math notranslate nohighlight">\(i\)</span> to agent <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(\Pi_X[]\)</span> denotes the projection operator over the set <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(\tilde{\nabla} f_i(y_i^k)\in\partial f_i(y_i^k)\)</span> a (sub)gradient of <span class="math notranslate nohighlight">\(f_i\)</span> computed at <span class="math notranslate nohighlight">\(y_i^k\)</span>.
The weight matrix <span class="math notranslate nohighlight">\(W=[w_{ij}]_{i,j=1}^N\)</span> should be doubly stochastic.</p>
<p>The algorithm, as written above, was originally presented in <a class="reference internal" href="#neoz09" id="id1"><span>[NeOz09]</span></a>. Many other variants and extension has been proposed, allowing for stochastic objective functions, time-varying graphs, local stepsize sequences. All these variant can be implemented through the <a class="reference internal" href="#disropt.algorithms.subgradient.SubgradientMethod" title="disropt.algorithms.subgradient.SubgradientMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">SubgradientMethod</span></code></a> class.</p>
<dl class="method">
<dt id="disropt.algorithms.subgradient.SubgradientMethod.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">iterations=1000</em>, <em class="sig-param">stepsize=0.001</em>, <em class="sig-param">verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/subgradient.html#SubgradientMethod.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.subgradient.SubgradientMethod.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the algorithm for a given number of iterations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>iterations</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of iterations. Defaults to 1000.</p></li>
<li><p><strong>stepsize</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – If a float is given as input, the stepsize is constant.
If a function is given, it must take an iteration k as input and output the corresponding stepsize.. Defaults to 0.1.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True print some information during the evolution of the algorithm. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – The number of iterations must be an int</p></li>
<li><p><strong>TypeError</strong> – The stepsize must be a float or a callable</p></li>
<li><p><strong>ValueError</strong> – Only sets (children of AstractSet) with explicit projections are currently supported</p></li>
<li><p><strong>ValueError</strong> – Only one constraint per time is currently supported</p></li>
</ul>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>return the sequence of estimates if enable_log=True.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="randomized-block-sub-gradient-method">
<h2>Randomized Block (Sub)gradient Method<a class="headerlink" href="#randomized-block-sub-gradient-method" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="disropt.algorithms.subgradient.BlockSubgradientMethod">
<em class="property">class </em><code class="sig-prename descclassname">disropt.algorithms.subgradient.</code><code class="sig-name descname">BlockSubgradientMethod</code><span class="sig-paren">(</span><em class="sig-param">agent</em>, <em class="sig-param">initial_condition</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/subgradient.html#BlockSubgradientMethod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.subgradient.BlockSubgradientMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="disropt.algorithms.consensus.html#disropt.algorithms.consensus.BlockConsensus" title="disropt.algorithms.consensus.BlockConsensus"><code class="xref py py-class docutils literal notranslate"><span class="pre">disropt.algorithms.consensus.BlockConsensus</span></code></a></p>
<p>Distributed block subgradient method. This is a special instance of the Block Proximal Method in <a class="reference internal" href="#fano19" id="id2"><span>[FaNo19]</span></a></p>
<p>At each iteration, the agent can update its local estimate or not at each iteration according to a certain probability (awakening_probability).
From the perspective of agent <span class="math notranslate nohighlight">\(i\)</span> the algorithm works as follows. At iteration <span class="math notranslate nohighlight">\(k\)</span> if the agent is awake, it selects a random block <span class="math notranslate nohighlight">\(\ell_i^k\)</span> of its local solution and updates</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
y_i^k &amp;= \sum_{j\in\mathcal{N}_i} w_{ij} x_{j\mid i}^k \\
x_{i,\ell}^{k+1} &amp;= \begin{cases}
     \Pi_{X_\ell}\left[y_i^k - \alpha_i^k [\tilde{\nabla} f_i(y_i^k)]_{\ell}\right] &amp; \text{if } \ell = \ell_i^k \\
     x_{i,\ell}^{k} &amp; \text{otherwise}
     \end{cases}
\end{align}\end{split}\]</div>
<p>then it broadcasts <span class="math notranslate nohighlight">\(x_{i,\ell_i^k}^{k+1}\)</span> to its out-neighbors. Otherwise (if the agent is not awake) <span class="math notranslate nohighlight">\(x_{i}^{k+1}=x_i^k\)</span>.
Here <span class="math notranslate nohighlight">\(\mathcal{N}_i\)</span> is the current set of in-neighbors and <span class="math notranslate nohighlight">\(x_{j\mid i},j\in\mathcal{N}_i\)</span> is the local copy of <span class="math notranslate nohighlight">\(x_j\)</span> available at node <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(x_{i,\ell}\)</span> denotes the <span class="math notranslate nohighlight">\(\ell\)</span>-th block of <span class="math notranslate nohighlight">\(x_i\)</span>.
The weight matrix <span class="math notranslate nohighlight">\(W=[w_{ij}]_{i,j=1}^N\)</span> should be doubly stochastic.</p>
<p>Notice that if there is only one block and awakening_probability=1 the <a class="reference internal" href="#disropt.algorithms.subgradient.BlockSubgradientMethod" title="disropt.algorithms.subgradient.BlockSubgradientMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockSubgradientMethod</span></code></a> reduces to the <a class="reference internal" href="#disropt.algorithms.subgradient.SubgradientMethod" title="disropt.algorithms.subgradient.SubgradientMethod"><code class="xref py py-class docutils literal notranslate"><span class="pre">SubgradientMethod</span></code></a>.</p>
<dl class="method">
<dt id="disropt.algorithms.subgradient.BlockSubgradientMethod.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">iterations=1000</em>, <em class="sig-param">stepsize=0.1</em>, <em class="sig-param">verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/subgradient.html#BlockSubgradientMethod.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.subgradient.BlockSubgradientMethod.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the algorithm for a given number of iterations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>iterations</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of iterations. Defaults to 1000.</p></li>
<li><p><strong>stepsize</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – If a float is given as input, the stepsize is constant.
If a function is given, it must take an iteration k as input and output the corresponding stepsize.. Defaults to 0.1.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True print some information during the evolution of the algorithm. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – The number of iterations must be an int</p></li>
<li><p><strong>TypeError</strong> – The stepsize must be a float or a callable</p></li>
<li><p><strong>ValueError</strong> – Only sets (children of AstractSet) with explicit projections are currently supported</p></li>
<li><p><strong>ValueError</strong> – Only one constraint per time is currently supported</p></li>
</ul>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>return the sequence of estimates if enable_log=True.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="distributed-gradient-tracking">
<h2>Distributed Gradient Tracking<a class="headerlink" href="#distributed-gradient-tracking" title="Permalink to this headline">¶</a></h2>
<span class="target" id="alg-gradient-tracking"></span><dl class="class">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking">
<em class="property">class </em><code class="sig-prename descclassname">disropt.algorithms.gradient_tracking.</code><code class="sig-name descname">GradientTracking</code><span class="sig-paren">(</span><em class="sig-param">agent</em>, <em class="sig-param">initial_condition</em>, <em class="sig-param">enable_log=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#GradientTracking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">disropt.algorithms.algorithm.Algorithm</span></code></p>
<p>Gradient Tracking Algorithm […]_</p>
<p>From the perspective of agent <span class="math notranslate nohighlight">\(i\)</span> the algorithm works as follows. For <span class="math notranslate nohighlight">\(k=0,1,\dots\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}x_i^{k+1} &amp; = \sum_{j=1}^N w_{ij} x_j^k - \alpha d_i^k \\
d_i^{k+1} &amp; = \sum_{j=1}^N w_{ij} d_j^k - [ \nabla f_i (x_i^{k+1}) - \nabla f_i (x_i^k)]\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\in\mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(d_i\in\mathbb{R}^n\)</span>. The weight matrix <span class="math notranslate nohighlight">\(W=[w_{ij}]\)</span> must be doubly-stochastic. Extensions to other class of weight matrices <span class="math notranslate nohighlight">\(W\)</span> are not currently supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>agent</strong> (<a class="reference internal" href="disropt.agents.html#disropt.agents.Agent" title="disropt.agents.Agent"><em>Agent</em></a>) – agent to execute the algorithm</p></li>
<li><p><strong>initial_condition</strong> (<em>numpy.ndarray</em>) – initial condition for <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p><strong>enable_log</strong> (<em>bool</em>) – True for enabling log</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.agent">
<code class="sig-name descname">agent</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.agent" title="Permalink to this definition">¶</a></dt>
<dd><p>agent to execute the algorithm</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="disropt.agents.html#disropt.agents.Agent" title="disropt.agents.Agent">Agent</a></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.x0">
<code class="sig-name descname">x0</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.x0" title="Permalink to this definition">¶</a></dt>
<dd><p>initial condition</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.x">
<code class="sig-name descname">x</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.x" title="Permalink to this definition">¶</a></dt>
<dd><p>current value of the local solution</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.d">
<code class="sig-name descname">d</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.d" title="Permalink to this definition">¶</a></dt>
<dd><p>current value of the local tracker</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.shape">
<code class="sig-name descname">shape</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>shape of the variable</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.x_neigh">
<code class="sig-name descname">x_neigh</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.x_neigh" title="Permalink to this definition">¶</a></dt>
<dd><p>dictionary containing the local solution of the (in-)neighbors</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.d_neigh">
<code class="sig-name descname">d_neigh</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.d_neigh" title="Permalink to this definition">¶</a></dt>
<dd><p>dictionary containing the local tracker of the (in-)neighbors</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.enable_log">
<code class="sig-name descname">enable_log</code><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.enable_log" title="Permalink to this definition">¶</a></dt>
<dd><p>True for enabling log</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.get_result">
<code class="sig-name descname">get_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#GradientTracking.get_result"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.get_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the actual value of x</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>value of x</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.iterate_run">
<code class="sig-name descname">iterate_run</code><span class="sig-paren">(</span><em class="sig-param">stepsize</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#GradientTracking.iterate_run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.iterate_run" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a single iterate of the gradient tracking algorithm</p>
</dd></dl>

<dl class="method">
<dt id="disropt.algorithms.gradient_tracking.GradientTracking.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">iterations=1000</em>, <em class="sig-param">stepsize=0.1</em>, <em class="sig-param">verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#GradientTracking.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.GradientTracking.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the gradient tracking algorithm for a given number of iterations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>iterations</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of iterations. Defaults to 1000.</p></li>
<li><p><strong>stepsize</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – If a float is given as input, the stepsize is constant.
Default is 0.01.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If True print some information during the evolution of the algorithm. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – The number of iterations must be an int</p></li>
<li><p><strong>TypeError</strong> – The stepsize must be a float</p></li>
</ul>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>return the sequence of estimates if enable_log=True.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="distributed-gradient-tracking-over-directed-unbalanced-graphs">
<h2>Distributed Gradient Tracking (over directed, unbalanced graphs)<a class="headerlink" href="#distributed-gradient-tracking-over-directed-unbalanced-graphs" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="disropt.algorithms.gradient_tracking.DirectedGradientTracking">
<em class="property">class </em><code class="sig-prename descclassname">disropt.algorithms.gradient_tracking.</code><code class="sig-name descname">DirectedGradientTracking</code><span class="sig-paren">(</span><em class="sig-param">agent</em>, <em class="sig-param">initial_condition</em>, <em class="sig-param">enable_log=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#DirectedGradientTracking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.DirectedGradientTracking" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="disropt.algorithms.consensus.html#disropt.algorithms.consensus.PushSumConsensus" title="disropt.algorithms.consensus.PushSumConsensus"><code class="xref py py-class docutils literal notranslate"><span class="pre">disropt.algorithms.consensus.PushSumConsensus</span></code></a></p>
<p>Gradient Tracking Algorithm <a class="reference internal" href="#xikh18" id="id3"><span>[XiKh18]</span></a></p>
<p>From the perspective of agent <span class="math notranslate nohighlight">\(i\)</span> the algorithm works as follows. For <span class="math notranslate nohighlight">\(k=0,1,\dots\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}x_i^{k+1} &amp; = \sum_{j=1}^N a_{ij} x_j^k - \alpha y_i^k \\
y_i^{k+1} &amp; = \sum_{j=1}^N b_{ij} (y_j^k - [ \nabla f_j (x_j^{k+1}) - \nabla f_j (x_j^k)])\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\in\mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(y_i\in\mathbb{R}^n\)</span>. The weight matrix <span class="math notranslate nohighlight">\(A=[a_{ij}]\)</span> must be row-stochastic, while <span class="math notranslate nohighlight">\(B=[b_{ij}]\)</span> must be column-stochastic. The underlying graph can be directed (and unbalanced).</p>
<dl class="method">
<dt id="disropt.algorithms.gradient_tracking.DirectedGradientTracking.get_result">
<code class="sig-name descname">get_result</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#DirectedGradientTracking.get_result"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.DirectedGradientTracking.get_result" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the actual value of x</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>value of x</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="disropt.algorithms.gradient_tracking.DirectedGradientTracking.iterate_run">
<code class="sig-name descname">iterate_run</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/disropt/algorithms/gradient_tracking.html#DirectedGradientTracking.iterate_run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#disropt.algorithms.gradient_tracking.DirectedGradientTracking.iterate_run" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a single iterate of the algorithm</p>
</dd></dl>

</dd></dl>

<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="neoz09"><span class="brackets"><a class="fn-backref" href="#id1">NeOz09</a></span></dt>
<dd><p>Nedic, Angelia; Asuman Ozdaglar: Distributed subgradient methods for multi-agent optimization: IEEE Transactions on Automatic Control 54.1 (2009): 48.</p>
</dd>
<dt class="label" id="fano19"><span class="brackets"><a class="fn-backref" href="#id2">FaNo19</a></span></dt>
<dd><p>Farina, Francesco, and Notarstefano, Giuseppe. Randomized Block Proximal Methods for Distributed Stochastic Big-Data Optimization. arXiv preprint arXiv:1905.04214 (2019).</p>
</dd>
<dt class="label" id="xikh18"><span class="brackets"><a class="fn-backref" href="#id3">XiKh18</a></span></dt>
<dd><p>Xin, Ran, and Usman A. Khan: A linear algorithm for optimization over directed graphs with geometric convergence. IEEE Control Systems Letters 2.3 (2018): 315-320.</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="disropt.algorithms.dual.html" class="btn btn-neutral float-right" title="Dual and Primal/Dual algorithms" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="disropt.algorithms.consensus.html" class="btn btn-neutral float-left" title="Consensus Algorithms" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, OPT4SMART

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>